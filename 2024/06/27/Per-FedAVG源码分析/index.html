<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Per_FedAVG源码分析 | 龙锦</title><meta name="author" content="龙金伟"><meta name="copyright" content="龙金伟"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Per_FedAVG 一个网上的实现的源码的注释和分析">
<meta property="og:type" content="article">
<meta property="og:title" content="Per_FedAVG源码分析">
<meta property="og:url" content="https://longjinw.github.io/2024/06/27/Per-FedAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="龙锦">
<meta property="og:description" content="Per_FedAVG 一个网上的实现的源码的注释和分析">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://longjinw.github.io/image/T1.jpg">
<meta property="article:published_time" content="2024-06-27T12:24:17.000Z">
<meta property="article:modified_time" content="2024-06-27T12:29:42.137Z">
<meta property="article:author" content="龙金伟">
<meta property="article:tag" content="Per_FedAVG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://longjinw.github.io/image/T1.jpg"><link rel="shortcut icon" href="/image/luogic.jpg"><link rel="canonical" href="https://longjinw.github.io/2024/06/27/Per-FedAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Per_FedAVG源码分析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-27 20:29:42'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><script src="/styles/fish.js"></script><script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script><link rel="stylesheet" href="/styles/main.css"><meta name="generator" content="Hexo 7.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/luogic.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/image/T1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="龙锦"><img class="site-icon" src="/image/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_202406231044301.jpg"/><span class="site-name">龙锦</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Per_FedAVG源码分析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-27T12:24:17.000Z" title="发表于 2024-06-27 20:24:17">2024-06-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-27T12:29:42.137Z" title="更新于 2024-06-27 20:29:42">2024-06-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/">python</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>32分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Per_FedAVG源码分析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="Per-FeAVG源码分析——根目录下：">Per_FeAVG源码分析——根目录下：</h3>
<p>KarhouTam的Per_FedAVG.源码链接：<a target="_blank" rel="noopener" href="https://github.com/KarhouTam/Per-FedAvg">请使用到的点个star</a></p>
<h4 id="utils-py"><a target="_blank" rel="noopener" href="http://utils.py">utils.py</a></h4>
<h5 id="函数：get-args（）">函数：get_args（）</h5>
<p>功能：用于加载参数:使用ArgumentParser()输入了<strong>联邦参数</strong>，<strong>模型参数</strong>，<strong>其他参数</strong>三类参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Iterator, <span class="type">Tuple</span>, <span class="type">Union</span></span><br><span class="line"><span class="keyword">from</span> argparse <span class="keyword">import</span> ArgumentParser</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_args</span>():</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    <span class="comment">##‘--alpha’表示参数名称，type代表参数类型，default代表默认值设置，help则是对alpha的描述性解释。</span></span><br><span class="line">    parser.add_argument(<span class="string">&quot;--alpha&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1e-2</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--beta&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1e-3</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--global_epochs&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">200</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--local_epochs&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">4</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--pers_epochs&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">1</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Indicate how many data batches would be used for personalization. Negatives means that equal to train phase.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--hf&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">0</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;0 for performing Per-FedAvg(FO), others for Per-FedAvg(HF)&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">40</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--valset_ratio&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">float</span>,</span><br><span class="line">        default=<span class="number">0.1</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Proportion of val set in the entire client local dataset&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dataset&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=[<span class="string">&quot;mnist&quot;</span>, <span class="string">&quot;cifar&quot;</span>], default=<span class="string">&quot;mnist&quot;</span></span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--client_num_per_round&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--seed&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">17</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--gpu&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">1</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Non-zero value for using gpu, 0 for using cpu&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--eval_while_training&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">1</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Non-zero value for performing local evaluation before and after local training&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--log&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> parser.parse_args() <span class="comment">#解析了命令行参数，并将解析结果作为函数的返回值，以便在其他地方可以使用参数</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="函数：eval（）">函数：eval（）</h5>
<p>功能：用于在PyTorch中评估给定模型的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad() </span><span class="comment">#这个装饰器确保在评估模型时不会计算梯度，从而节省内存和计算资源。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval</span>(<span class="params"></span></span><br><span class="line"><span class="params">    model: torch.nn.Module,<span class="comment">#评价的模型</span></span></span><br><span class="line"><span class="params">    dataloader: torch.utils.data.DataLoader,<span class="comment">#数据集加载器</span></span></span><br><span class="line"><span class="params">    criterion: <span class="type">Union</span>[torch.nn.MSELoss, torch.nn.CrossEntropyLoss],<span class="comment">#损失函数，可以是均方误差（MSE）或交叉熵损失。</span></span></span><br><span class="line"><span class="params">    device=torch.device(<span class="params"><span class="string">&quot;cpu&quot;</span></span>),<span class="comment">#用于运行模型的设备（默认为cpu）</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[torch.Tensor, torch.Tensor]:</span><br><span class="line">    <span class="comment">#将模型设置为评估模式，确保如Dropout或BatchNorm这样的层在评估时以不同的方式工作。</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    num_samples = <span class="number">0</span></span><br><span class="line">    acc = <span class="number">0</span></span><br><span class="line">    <span class="comment">#对于数据加载器中的每一批数据，计算损失和准确率。</span></span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> dataloader:</span><br><span class="line">        x, y = x.to(device), y.to(device)</span><br><span class="line">        logit = model(x)</span><br><span class="line">        <span class="comment"># total_loss += criterion(logit, y) / y.size(-1)</span></span><br><span class="line">        total_loss += criterion(logit, y) <span class="comment">#使用给定的损失函数计算预测（logit）和真实标签（y）之间的损失。</span></span><br><span class="line">        pred = torch.softmax(logit, -<span class="number">1</span>).argmax(-<span class="number">1</span>)<span class="comment">#使用 torch.softmax 和 argmax 获取预测的类别索引，然后与真实标签比较，统计正确的预测。</span></span><br><span class="line">        acc += torch.eq(pred, y).<span class="built_in">int</span>().<span class="built_in">sum</span>()</span><br><span class="line">        num_samples += y.size(-<span class="number">1</span>)</span><br><span class="line">    model.train()<span class="comment">#在评估结束后将模型重置为训练模式，尽管这通常不是必需的，因为下一个使用模型的操作可能会自动设置它。</span></span><br><span class="line">    <span class="keyword">return</span> total_loss, acc / num_samples</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="函数：fix-random-seed-seed-int">函数：fix_random_seed(seed: int)</h5>
<p>作用：设置随机种子以确保结果的可复现性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fix_random_seed</span>(<span class="params">seed: <span class="built_in">int</span></span>):</span><br><span class="line">    torch.cuda.empty_cache()<span class="comment">#这个函数会清空CUDA缓存。</span></span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed(seed)<span class="comment">#这分别设置CPU和GPU上的随机种子，以确保PyTorch操作的随机性是可复现的。这是正确的。</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)<span class="comment">#这两个函数分别设置Python标准库中的random模块和NumPy库中的随机数生成器的种子。这也是为了确保其他库中的随机操作是可复现的。</span></span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span><span class="comment">#当你设置deterministic=True时，你告诉cuDNN（CUDA Deep Neural Network library）在卷积操作中使用确定性的算法，而不是可能更快但不太确定的算法。这有助于确保即使在GPU上，结果也是可复现的。</span></span><br><span class="line">    torch.backends.cudnn.benchmark = <span class="literal">True</span><span class="comment">#这个设置告诉cuDNN为特定的配置自动寻找最快的卷积算法。但是，当benchmark=True时，cuDNN会尝试不同的算法并保留最佳的一个，这可能会导致结果不可复现，因为每次运行都可能选择不同的算法。</span></span><br></pre></td></tr></table></figure>
<h4 id="model-py"><a target="_blank" rel="noopener" href="http://model.py">model.py</a></h4>
<h5 id="函数：elu-nn-Module">函数：elu(nn.Module)</h5>
<p>作用：它实现了指数线性单元（Exponential Linear Unit, ELU）激活函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">elu</span>(nn.Module):<span class="comment">#继承自nn.Module的类elu。</span></span><br><span class="line">    <span class="comment">#调用了父类nn.Module的__init__方法来确保基类的初始化。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(elu, self).__init__()</span><br><span class="line">	<span class="comment">#实现了ELU激活函数。对于输入x，如果x大于或等于0，则返回x本身；否则，返回0.2 * (torch.exp(x) - 1)。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.where(x &gt;= <span class="number">0</span>, x, <span class="number">0.2</span> * (torch.exp(x) - <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h5 id="类：linear-nn-Module">类：linear(nn.Module)</h5>
<p>作用：<code>__init__</code>方法用于初始化权重（<code>w</code>）和偏置（<code>b</code>），而<code>forward</code>方法定义了数据通过网络层的前向传播过程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">linear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_c, out_c</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(linear, self).__init__()</span><br><span class="line">        <span class="comment">#使用了torch.randn(out_c, in_c) * torch.sqrt(torch.tensor(2 / in_c))来初始化权重，这是He初始化（也称为Kaiming初始化）的一个变种，它通常用于ReLU或其变种激活函数。</span></span><br><span class="line">        self.w = nn.Parameter(</span><br><span class="line">            torch.randn(out_c, in_c) * torch.sqrt(torch.tensor(<span class="number">2</span> / in_c))</span><br><span class="line">        )</span><br><span class="line">        self.b = nn.Parameter(torch.randn(out_c))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> F.linear(x, self.w, self.b)</span><br></pre></td></tr></table></figure>
<h5 id="类：MLP-MNIST">类：MLP_MNIST</h5>
<p>作用：构建不同类型的神经网络模型，分别是MLP（多层感知机）、CNNMnist（用于MNIST手写数字数据集的卷积神经网络）和CNNCifar（用于CIFAR-10数据集的卷积神经网络）。实现了神经网络的前向传播过程，并用于分类任务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP_MNIST</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(MLP_MNIST, self).__init__()</span><br><span class="line">        self.fc1 = linear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">80</span>)</span><br><span class="line">        self.fc2 = linear(<span class="number">80</span>, <span class="number">60</span>)</span><br><span class="line">        self.fc3 = linear(<span class="number">60</span>, <span class="number">10</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.activation = elu()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MLP_CIFAR10</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(MLP_CIFAR10, self).__init__()</span><br><span class="line">        self.fc1 = linear(<span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>, <span class="number">80</span>)</span><br><span class="line">        self.fc2 = linear(<span class="number">80</span>, <span class="number">60</span>)</span><br><span class="line">        self.fc3 = linear(<span class="number">60</span>, <span class="number">10</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.activation = elu()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h5 id="类：MLP-CIFAR10">类：MLP_CIFAR10</h5>
<p>作用：构建不同类型的神经网络模型，分别是MLP（多层感知机）、CNNMnist（用于MNIST手写数字数据集的卷积神经网络）和CNNCifar（用于CIFAR-10数据集的卷积神经网络）。实现了神经网络的前向传播过程，并用于分类任务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP_CIFAR10</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">super</span>(MLP_CIFAR10, self).__init__()</span><br><span class="line">        self.fc1 = linear(<span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>, <span class="number">80</span>)</span><br><span class="line">        self.fc2 = linear(<span class="number">80</span>, <span class="number">60</span>)</span><br><span class="line">        self.fc3 = linear(<span class="number">60</span>, <span class="number">10</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.activation = elu()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        x = self.activation(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h5 id="字典：MODEL-DICT">字典：MODEL_DICT</h5>
<p>作用：关联键<code>&quot;mnist&quot;</code>和<code>&quot;cifar&quot;</code>到它们各自的多层感知机（MLP）模型类<code>MLP_MNIST</code>和<code>MLP_CIFAR10</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MODEL_DICT = &#123;<span class="string">&quot;mnist&quot;</span>: MLP_MNIST, <span class="string">&quot;cifar&quot;</span>: MLP_CIFAR10&#125;</span><br></pre></td></tr></table></figure>
<h5 id="函数：get-model-dataset-device">函数：get_model(dataset, device)</h5>
<p>作用：根据数据集名称从<code>MODEL_DICT</code>字典中获取相应的模型类，并实例化模型，然后将模型移动到指定的设备上（CPU或GPU）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_model</span>(<span class="params">dataset, device</span>):</span><br><span class="line">    <span class="keyword">return</span> MODEL_DICT[dataset]().to(device)</span><br></pre></td></tr></table></figure>
<h4 id="perfedavg-py"><a target="_blank" rel="noopener" href="http://perfedavg.py">perfedavg.py</a></h4>
<h5 id="函数：init">函数：<strong>init</strong>()</h5>
<p>作用：类的初始化方法，用于配置和初始化类的实例变量。该方法接收多个参数，包括客户端ID、学习率参数（alpha和beta，可能是某种优化算法中的参数，如Momentum或Adam中的beta1和beta2）、全局模型、损失函数、批量大小、数据集名称、本地训练轮数、验证集比例、日志记录器和GPU设备ID。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    client_id: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    alpha: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    beta: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    global_model: torch.nn.Module,</span></span><br><span class="line"><span class="params">    criterion: <span class="type">Union</span>[torch.nn.CrossEntropyLoss, torch.nn.MSELoss],</span></span><br><span class="line"><span class="params">    batch_size: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    dataset: <span class="built_in">str</span>,</span></span><br><span class="line"><span class="params">    local_epochs: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    valset_ratio: <span class="built_in">float</span>,</span></span><br><span class="line"><span class="params">    logger: rich.console.Console,</span></span><br><span class="line"><span class="params">    gpu: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment">#设备选择：根据传入的gpu参数和torch.cuda.is_available()的结果，选择使用CPU还是GPU进行计算。</span></span><br><span class="line">    <span class="keyword">if</span> gpu <span class="keyword">and</span> torch.cuda.is_available():</span><br><span class="line">        self.device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    <span class="comment">#日志记录器：将传入的logger实例保存在类的实例变量中，以便在类的其他方法中使用。</span></span><br><span class="line">    self.logger = logger</span><br><span class="line">	<span class="comment">#本地训练参数：保存了本地训练轮数（local_epochs）和损失函数（criterion）。</span></span><br><span class="line">    self.local_epochs = local_epochs</span><br><span class="line">    self.criterion = criterion</span><br><span class="line">    <span class="comment">#客户端ID和模型：保存了客户端ID（client_id）和全局模型的深拷贝（global_model）。</span></span><br><span class="line">    self.<span class="built_in">id</span> = client_id</span><br><span class="line">    self.model = deepcopy(global_model)</span><br><span class="line">    <span class="comment">#学习率参数：保存了alpha和beta参数，这些参数可能是优化算法的一部分。</span></span><br><span class="line">    self.alpha = alpha</span><br><span class="line">    self.beta = beta</span><br><span class="line">    <span class="comment">#数据加载器：调用get_dataloader函数来获取训练和验证的数据加载器（trainloader和valloader）。这个函数根据数据集名称、客户端ID、批量大小和验证集比例来返回相应的数据加载器。</span></span><br><span class="line">    self.trainloader, self.valloader = get_dataloader(</span><br><span class="line">        dataset, client_id, batch_size, valset_ratio</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#迭代训练加载器：将训练数据加载器转换为迭代器并保存在iter_trainloader中。这样做可能是为了在类的其他方法中方便地从训练集中获取批量数据。</span></span><br><span class="line">    self.iter_trainloader = <span class="built_in">iter</span>(self.trainloader)</span><br></pre></td></tr></table></figure>
<h5 id="函数：get-data-batch-self">函数：get_data_batch(self)</h5>
<p>作用：用于从训练数据加载器中获取下一批数据，并处理<code>StopIteration</code>异常（当迭代器耗尽时触发）。当<code>iter_trainloader</code>中的数据被完全迭代一遍后，该方法会重新初始化迭代器并获取新的数据批次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_data_batch</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="comment">#尝试获取数据：try块尝试从self.iter_trainloader（一个迭代器）中获取下一批数据（x为输入数据，y为标签）</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        x, y = <span class="built_in">next</span>(self.iter_trainloader)</span><br><span class="line">     <span class="comment">#异常处理：</span></span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        self.iter_trainloader = <span class="built_in">iter</span>(self.trainloader)<span class="comment">#重新初始化iter_trainloader，通过调用iter(self.trainloader)来创建一个新的迭代器。</span></span><br><span class="line">        x, y = <span class="built_in">next</span>(self.iter_trainloader)<span class="comment">#再次尝试从新的迭代器中获取下一批数据。</span></span><br><span class="line">	<span class="comment">#无论数据是直接从原始迭代器中获取，还是通过重新初始化迭代器后获取，都会将数据（x和y）移动到self.device（即CPU或GPU）上，并返回它们。</span></span><br><span class="line">    <span class="keyword">return</span> x.to(self.device), y.to(self.device)</span><br></pre></td></tr></table></figure>
<h5 id="函数：train（）">函数：train（）</h5>
<p>作用：是一个用于在本地客户端上训练模型的函数。该方法接收全局模型、一个布尔值<code>hessian_free</code>（用于指示是否使用Hessian-free优化）和一个布尔值<code>eval_while_training</code>（用于指示是否在训练前后评估模型性能）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    global_model: torch.nn.Module,</span></span><br><span class="line"><span class="params">    hessian_free=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    eval_while_training=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    self.model.load_state_dict(global_model.state_dict())</span><br><span class="line">    <span class="comment">#训练前评估（可选）：如果eval_while_training为True，则在训练开始前使用utils.eval函数评估模型在验证集上的性能，并保存损失和准确率。</span></span><br><span class="line">    <span class="keyword">if</span> eval_while_training:</span><br><span class="line">        loss_before, acc_before = utils.<span class="built_in">eval</span>(</span><br><span class="line">            self.model, self.valloader, self.criterion, self.device</span><br><span class="line">        )</span><br><span class="line">    <span class="comment">#执行训练：调用_train方法</span></span><br><span class="line">    self._train(hessian_free)</span><br><span class="line">    </span><br><span class="line">	<span class="comment">#训练后评估（可选）：如果eval_while_training为True，则在训练结束后再次使用utils.eval函数评估模型在验证集上的性能，并保存损失和准确率。</span></span><br><span class="line">    <span class="keyword">if</span> eval_while_training:</span><br><span class="line">        loss_after, acc_after = utils.<span class="built_in">eval</span>(</span><br><span class="line">            self.model, self.valloader, self.criterion, self.device</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#记录并返回模型：使用self.logger记录训练前后的损失和准确率变化。然后，使用SerializationTool.serialize_model方法将训练后的模型序列化为某种格式，并返回该序列化模型。</span></span><br><span class="line">        self.logger.log(</span><br><span class="line">            <span class="string">&quot;client [&#123;&#125;] [red]loss: &#123;:.4f&#125; -&gt; &#123;:.4f&#125;   [blue]acc: &#123;:.2f&#125;% -&gt; &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                self.<span class="built_in">id</span>,</span><br><span class="line">                loss_before,</span><br><span class="line">                loss_after,</span><br><span class="line">                acc_before * <span class="number">100.0</span>,</span><br><span class="line">                acc_after * <span class="number">100.0</span>,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> SerializationTool.serialize_model(self.model)</span><br></pre></td></tr></table></figure>
<h5 id="函数：-train">函数：_train()</h5>
<p>作用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">_train</span>(<span class="params">self, hessian_free=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment">#使用Hessian-free方法的Per-FedAvg（HF）</span></span><br><span class="line">    <span class="comment">#当hessian_free为True时，该方法将执行Hessian-free的Per-FedAvg训练过程。这通常涉及计算二阶导数（Hessian）的近似，以优化模型参数。</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">if</span> hessian_free:  <span class="comment"># Per-FedAvg(HF)</span></span><br><span class="line">        <span class="comment">#对于每个本地训练周期（self.local_epochs），首先复制当前模型（self.model）到一个临时模型（temp_model）中。</span></span><br><span class="line">        <span class="comment">#使用get_data_batch方法获取第一批数据（data_batch_1），并计算关于临时模型的一阶梯度（grads）。</span></span><br><span class="line">        <span class="comment">#使用这些梯度来更新临时模型的参数（这里使用了简单的SGD更新，但学习率self.alpha可能需要根据实际情况调整）。</span></span><br><span class="line">        <span class="comment">#接着，获取第二批数据（data_batch_2），并再次计算关于临时模型的一阶梯度（grads_1st）。</span></span><br><span class="line">        <span class="comment">#然后，获取第三批数据（data_batch_3），但这次计算的是关于原始模型（self.model）的二阶梯度（Hessian向量积，即grads_2nd）。注意，这里的计算可能需要特定的函数或库，因为直接计算完整的Hessian矩阵是计算密集且不可行的。</span></span><br><span class="line">        <span class="comment">#最后，使用这些一阶梯度和二阶梯度来更新原始模型的参数。更新规则似乎结合了梯度下降和二阶优化方法（具体是哪种方法取决于self.beta和self.alpha的值）。</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.local_epochs):</span><br><span class="line">            temp_model = deepcopy(self.model)</span><br><span class="line">            data_batch_1 = self.get_data_batch()</span><br><span class="line">            grads = self.compute_grad(temp_model, data_batch_1)</span><br><span class="line">            <span class="keyword">for</span> param, grad <span class="keyword">in</span> <span class="built_in">zip</span>(temp_model.parameters(), grads):</span><br><span class="line">                param.data.sub_(self.alpha * grad)</span><br><span class="line"></span><br><span class="line">            data_batch_2 = self.get_data_batch()</span><br><span class="line">            grads_1st = self.compute_grad(temp_model, data_batch_2)</span><br><span class="line"></span><br><span class="line">            data_batch_3 = self.get_data_batch()</span><br><span class="line"></span><br><span class="line">            grads_2nd = self.compute_grad(</span><br><span class="line">                self.model, data_batch_3, v=grads_1st, second_order_grads=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># <span class="doctag">NOTE:</span> Go check https://github.com/KarhouTam/Per-FedAvg/issues/2 if you confuse about the model update.</span></span><br><span class="line">            <span class="keyword">for</span> param, grad1, grad2 <span class="keyword">in</span> <span class="built_in">zip</span>(</span><br><span class="line">                self.model.parameters(), grads_1st, grads_2nd</span><br><span class="line">            ):</span><br><span class="line">                param.data.sub_(self.beta * grad1 - self.beta * self.alpha * grad2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># Per-FedAvg(FO）</span></span><br><span class="line">        	<span class="comment">#只使用一阶梯度的Per-FedAvg（FO）</span></span><br><span class="line">            <span class="comment">#首先在一个临时模型（temp_model）上计算第一个数据批次（data_batch_1）的梯度，并更新临时模型的参数。</span></span><br><span class="line">            <span class="comment">#然后，它获取第二个数据批次（data_batch_2）并计算梯度，但这次它直接在原始模型（self.model）上应用这些梯度的更新，而不是临时模型。</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.local_epochs):</span><br><span class="line">            <span class="comment"># ========================== FedAvg ==========================</span></span><br><span class="line">            <span class="comment"># <span class="doctag">NOTE:</span> You can uncomment those codes for running FedAvg.</span></span><br><span class="line">            <span class="comment">#       When you&#x27;re trying to run FedAvg, comment other codes in this branch.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># data_batch = self.get_data_batch()</span></span><br><span class="line">            <span class="comment"># grads = self.compute_grad(self.model, data_batch)</span></span><br><span class="line">            <span class="comment"># for param, grad in zip(self.model.parameters(), grads):</span></span><br><span class="line">            <span class="comment">#     param.data.sub_(self.beta * grad)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># ============================================================</span></span><br><span class="line"></span><br><span class="line">            temp_model = deepcopy(self.model)</span><br><span class="line">            data_batch_1 = self.get_data_batch()</span><br><span class="line">            grads = self.compute_grad(temp_model, data_batch_1)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> param, grad <span class="keyword">in</span> <span class="built_in">zip</span>(temp_model.parameters(), grads):</span><br><span class="line">                param.data.sub_(self.alpha * grad)</span><br><span class="line"></span><br><span class="line">            data_batch_2 = self.get_data_batch()</span><br><span class="line">            grads = self.compute_grad(temp_model, data_batch_2)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> param, grad <span class="keyword">in</span> <span class="built_in">zip</span>(self.model.parameters(), grads):</span><br><span class="line">                param.data.sub_(self.beta * grad)</span><br></pre></td></tr></table></figure>
<h5 id="函数：compute-grad">函数：compute_grad()</h5>
<p>作用：根据给定的数据批次<code>data_batch</code>计算模型<code>model</code>的梯度。如果<code>second_order_grads</code>为<code>True</code>，它将计算二阶梯度（Hessian-vector积的一个近似），否则，它将计算标准的一阶梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_grad</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    model: torch.nn.Module,</span></span><br><span class="line"><span class="params">    data_batch: <span class="type">Tuple</span>[torch.Tensor, torch.Tensor],</span></span><br><span class="line"><span class="params">    v: <span class="type">Union</span>[<span class="type">Tuple</span>[torch.Tensor, ...], <span class="literal">None</span>] = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    second_order_grads=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    x, y = data_batch</span><br><span class="line">    <span class="keyword">if</span> second_order_grads:</span><br><span class="line">        frz_model_params = deepcopy(model.state_dict())</span><br><span class="line">        delta = <span class="number">1e-3</span></span><br><span class="line">        dummy_model_params_1 = OrderedDict()</span><br><span class="line">        dummy_model_params_2 = OrderedDict()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> (layer_name, param), grad <span class="keyword">in</span> <span class="built_in">zip</span>(model.named_parameters(), v):</span><br><span class="line">                dummy_model_params_1.update(&#123;layer_name: param + delta * grad&#125;)</span><br><span class="line">                dummy_model_params_2.update(&#123;layer_name: param - delta * grad&#125;)</span><br><span class="line"></span><br><span class="line">        model.load_state_dict(dummy_model_params_1, strict=<span class="literal">False</span>)</span><br><span class="line">        logit_1 = model(x)</span><br><span class="line">        loss_1 = self.criterion(logit_1, y)</span><br><span class="line">        grads_1 = torch.autograd.grad(loss_1, model.parameters())</span><br><span class="line"></span><br><span class="line">        model.load_state_dict(dummy_model_params_2, strict=<span class="literal">False</span>)</span><br><span class="line">        logit_2 = model(x)</span><br><span class="line">        loss_2 = self.criterion(logit_2, y)</span><br><span class="line">        grads_2 = torch.autograd.grad(loss_2, model.parameters())</span><br><span class="line"></span><br><span class="line">        model.load_state_dict(frz_model_params)</span><br><span class="line"></span><br><span class="line">        grads = []</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> g1, g2 <span class="keyword">in</span> <span class="built_in">zip</span>(grads_1, grads_2):</span><br><span class="line">                grads.append((g1 - g2) / (<span class="number">2</span> * delta))</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logit = model(x)</span><br><span class="line">        loss = self.criterion(logit, y)</span><br><span class="line">        grads = torch.autograd.grad(loss, model.parameters())</span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<h5 id="函数：pers-N-eval">函数：pers_N_eval()</h5>
<p>作用：在给定全局模型（<code>global_model</code>）和个性化训练轮次（<code>pers_epochs</code>）之后，该函数首先加载全局模型的参数到客户端的本地模型（<code>self.model</code>），然后在本地数据集上进行训练和评估。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pers_N_eval</span>(<span class="params">self, global_model: torch.nn.Module, pers_epochs: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="comment">#加载全局模型参数：</span></span><br><span class="line">    self.model.load_state_dict(global_model.state_dict())</span><br><span class="line">	</span><br><span class="line">    <span class="comment">#评估初始模型性能：</span></span><br><span class="line">    loss_before, acc_before = utils.<span class="built_in">eval</span>(</span><br><span class="line">        self.model, self.valloader, self.criterion, self.device</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#定义优化器：</span></span><br><span class="line">    optimizer = torch.optim.SGD(self.model.parameters(), lr=self.alpha)</span><br><span class="line">    <span class="comment">#个性化训练：这部分代码执行了 pers_epochs 轮次的个性化训练。在每次迭代中，它首先从 self.get_data_batch() 获取一个数据批次，然后使用这个数据批次来更新模型的参数。</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(pers_epochs):</span><br><span class="line">        x, y = self.get_data_batch()</span><br><span class="line">        logit = self.model(x)</span><br><span class="line">        loss = self.criterion(logit, y)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">     <span class="comment">#评估训练后的模型性能：在个性化训练完成后，再次评估模型的性能。</span></span><br><span class="line">    loss_after, acc_after = utils.<span class="built_in">eval</span>(</span><br><span class="line">        self.model, self.valloader, self.criterion, self.device</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#记录并输出日志：使用日志记录器（self.logger）来记录训练前后的损失和准确率。这里还使用了颜色代码（如 [red] 和 [blue]），但这些可能不会在纯文本日志中显示，除非日志记录器进</span></span><br><span class="line">    self.logger.log(</span><br><span class="line">        <span class="string">&quot;client [&#123;&#125;] [red]loss: &#123;:.4f&#125; -&gt; &#123;:.4f&#125;   [blue]acc: &#123;:.2f&#125;% -&gt; &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">            self.<span class="built_in">id</span>, loss_before, loss_after, acc_before * <span class="number">100.0</span>, acc_after * <span class="number">100.0</span></span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#返回评估结果：</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;loss_before&quot;</span>: loss_before,</span><br><span class="line">        <span class="string">&quot;acc_before&quot;</span>: acc_before,</span><br><span class="line">        <span class="string">&quot;loss_after&quot;</span>: loss_after,</span><br><span class="line">        <span class="string">&quot;acc_after&quot;</span>: acc_after,</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="main-py"><a target="_blank" rel="noopener" href="http://main.py">main.py</a></h4>
<p>用于启动分布式或联邦学习中的客户端或服务器进程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    </span><br><span class="line">    args = get_args()<span class="comment">#使用get_args()从命令行获取参数，并将这些参数存储在一个对象中</span></span><br><span class="line">    fix_random_seed(args.seed)<span class="comment">#用于设置随机种子，确保实验的可重复性。</span></span><br><span class="line">    <span class="keyword">if</span> os.path.isdir(<span class="string">&quot;./log&quot;</span>) == <span class="literal">False</span>:<span class="comment">#这行代码检查当前目录下是否存在一个名为log的目录。如果不存在，则执行下一行代码。</span></span><br><span class="line">        os.mkdir(<span class="string">&quot;./log&quot;</span>)<span class="comment">#如果log目录不存在，这行代码会创建它。os.mkdir用于创建新目录。</span></span><br><span class="line">    <span class="comment">#首先，检查args对象中是否有gpu参数且其值为True；其次，检查是否有可用的CUDA设备（即是否有NVIDIA GPU并安装了适当的CUDA和PyTorch版本）。</span></span><br><span class="line">    <span class="keyword">if</span> args.gpu <span class="keyword">and</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">     </span><br><span class="line">    global_model = get_model(args.dataset, device)<span class="comment">#创建了一个日志记录器对象logger</span></span><br><span class="line">    logger = Console(record=args.log)</span><br><span class="line">    logger.log(<span class="string">f&quot;Arguments:&quot;</span>, <span class="built_in">dict</span>(args._get_kwargs()))</span><br><span class="line">    clients_4_training, clients_4_eval, client_num_in_total = get_client_id_indices(</span><br><span class="line">        args.dataset</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># init clients </span></span><br><span class="line">    <span class="comment">#初始化一个客户端列表，每个客户端都是PerFedAvgClient类的实例。</span></span><br><span class="line">    clients = [</span><br><span class="line">        PerFedAvgClient(</span><br><span class="line">            client_id=client_id,</span><br><span class="line">            alpha=args.alpha,</span><br><span class="line">            beta=args.beta,</span><br><span class="line">            global_model=global_model,</span><br><span class="line">            criterion=torch.nn.CrossEntropyLoss(),</span><br><span class="line">            batch_size=args.batch_size,</span><br><span class="line">            dataset=args.dataset,</span><br><span class="line">            local_epochs=args.local_epochs,</span><br><span class="line">            valset_ratio=args.valset_ratio,</span><br><span class="line">            logger=logger,</span><br><span class="line">            gpu=args.gpu,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> client_id <span class="keyword">in</span> <span class="built_in">range</span>(client_num_in_total)</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    <span class="comment">#开始训练过程的，并且它使用了日志记录器（logger）来输出到log。</span></span><br><span class="line">    logger.log(<span class="string">&quot;=&quot;</span> * <span class="number">20</span>, <span class="string">&quot;TRAINING&quot;</span>, <span class="string">&quot;=&quot;</span> * <span class="number">20</span>, style=<span class="string">&quot;bold red&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> track(			<span class="comment">#全局训练循环:</span></span><br><span class="line">        <span class="built_in">range</span>(args.global_epochs), <span class="string">&quot;Training...&quot;</span>, console=logger, disable=args.log</span><br><span class="line">    ):</span><br><span class="line">        <span class="comment"># select clients   </span></span><br><span class="line">        <span class="comment">#选择客户端:在每次全局迭代中，代码从clients_4_training中随机选择args.client_num_per_round个客户端进行本地训练。</span></span><br><span class="line">        selected_clients = random.sample(clients_4_training, args.client_num_per_round)</span><br><span class="line"></span><br><span class="line">        model_params_cache = []</span><br><span class="line">        <span class="comment"># client local training 客户端本地训练</span></span><br><span class="line">        <span class="comment">#对于选定的每个客户端，代码执行train方法。该方法以当前的全局模型作为起点，并可能在本地数据集上进行训练。train方法返回序列化后的模型参数，这些参数被添加到model_params_cache列表中。</span></span><br><span class="line">        <span class="keyword">for</span> client_id <span class="keyword">in</span> selected_clients:</span><br><span class="line">            serialized_model_params = clients[client_id].train(</span><br><span class="line">                global_model=global_model,</span><br><span class="line">                hessian_free=args.hf,</span><br><span class="line">                eval_while_training=args.eval_while_training,</span><br><span class="line">            )</span><br><span class="line">            model_params_cache.append(serialized_model_params)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># aggregate model parameters聚合模型参数:</span></span><br><span class="line">        <span class="comment">#在所有选定的客户端完成本地训练后，代码使用fedavg_aggregate函数（来聚合模型参数。这个函数将model_params_cache列表中的模型参数进行聚合（通常使用FedAvg算法，即加权平均）。然后，使用deserialize_model函数将聚合后的模型参数反序列化并应用到global_model上，从而更新全局模型。</span></span><br><span class="line">        aggregated_model_params = Aggregators.fedavg_aggregate(model_params_cache)</span><br><span class="line">        SerializationTool.deserialize_model(global_model, aggregated_model_params)</span><br><span class="line">        <span class="comment">#分隔符日志:</span></span><br><span class="line">        <span class="comment">#最后，代码使用logger对象输出一个由60个等号字符组成的分隔符，可能用于在日志中分隔不同的全局迭代轮次</span></span><br><span class="line">        logger.log(<span class="string">&quot;=&quot;</span> * <span class="number">60</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#描述了联邦学习中的评估过程，并且它记录了模型在评估前后的性能。</span></span><br><span class="line">    <span class="comment"># eval</span></span><br><span class="line">    pers_epochs = args.local_epochs <span class="keyword">if</span> args.pers_epochs == -<span class="number">1</span> <span class="keyword">else</span> args.pers_epochs <span class="comment">#确定持久化轮次（Persistent Epochs）</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#开始评估日志</span></span><br><span class="line">    <span class="comment">#初始化评估结果列表</span></span><br><span class="line">    <span class="comment">#这四个列表用于存储每个客户端在评估前后的损失和准确率。</span></span><br><span class="line">    logger.log(<span class="string">&quot;=&quot;</span> * <span class="number">20</span>, <span class="string">&quot;EVALUATION&quot;</span>, <span class="string">&quot;=&quot;</span> * <span class="number">20</span>, style=<span class="string">&quot;bold blue&quot;</span>)</span><br><span class="line">    loss_before = []</span><br><span class="line">    loss_after = []</span><br><span class="line">    acc_before = []</span><br><span class="line">    acc_after = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#客户端评估循环</span></span><br><span class="line">    <span class="comment">#代码遍历clients_4_eval列表中的每个客户端ID，并对每个客户端执行pers_N_eval方法。这个方法在评估前对模型进行本地训练（使用pers_epochs指定的轮次），然后评估模型的性能，并返回一个包含损失和准确率的字典。这些值随后被添加到相应的列表中。</span></span><br><span class="line">    <span class="keyword">for</span> client_id <span class="keyword">in</span> track(</span><br><span class="line">        clients_4_eval, <span class="string">&quot;Evaluating...&quot;</span>, console=logger, disable=args.log</span><br><span class="line">    ):</span><br><span class="line">        stats = clients[client_id].pers_N_eval(</span><br><span class="line">            global_model=global_model, pers_epochs=pers_epochs,</span><br><span class="line">        )</span><br><span class="line">        loss_before.append(stats[<span class="string">&quot;loss_before&quot;</span>])</span><br><span class="line">        loss_after.append(stats[<span class="string">&quot;loss_after&quot;</span>])</span><br><span class="line">        acc_before.append(stats[<span class="string">&quot;acc_before&quot;</span>])</span><br><span class="line">        acc_after.append(stats[<span class="string">&quot;acc_after&quot;</span>])</span><br><span class="line">	</span><br><span class="line">    <span class="comment">#输出评估结果</span></span><br><span class="line">    <span class="comment">#代码使用logger对象输出评估结果。它计算了所有客户端的平均损失和准确率，并将它们以格式化的方式输出。</span></span><br><span class="line">    logger.log(<span class="string">&quot;=&quot;</span> * <span class="number">20</span>, <span class="string">&quot;RESULTS&quot;</span>, <span class="string">&quot;=&quot;</span> * <span class="number">20</span>, style=<span class="string">&quot;bold green&quot;</span>)</span><br><span class="line">    logger.log(<span class="string">f&quot;loss_before_pers: <span class="subst">&#123;(<span class="built_in">sum</span>(loss_before) / <span class="built_in">len</span>(loss_before)):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    logger.log(<span class="string">f&quot;acc_before_pers: <span class="subst">&#123;(<span class="built_in">sum</span>(acc_before) * <span class="number">100.0</span> / <span class="built_in">len</span>(acc_before)):<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line">    logger.log(<span class="string">f&quot;loss_after_pers: <span class="subst">&#123;(<span class="built_in">sum</span>(loss_after) / <span class="built_in">len</span>(loss_after)):<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    logger.log(<span class="string">f&quot;acc_after_pers: <span class="subst">&#123;(<span class="built_in">sum</span>(acc_after) * <span class="number">100.0</span> / <span class="built_in">len</span>(acc_after)):<span class="number">.2</span>f&#125;</span>%&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#保存评估结果（如果启用日志）</span></span><br><span class="line">    <span class="keyword">if</span> args.log:</span><br><span class="line">        algo = <span class="string">&quot;HF&quot;</span> <span class="keyword">if</span> args.hf <span class="keyword">else</span> <span class="string">&quot;FO&quot;</span></span><br><span class="line">        logger.save_html(</span><br><span class="line">            <span class="string">f&quot;./log/<span class="subst">&#123;args.dataset&#125;</span>_<span class="subst">&#123;args.client_num_per_round&#125;</span>_<span class="subst">&#123;args.global_epochs&#125;</span>_<span class="subst">&#123;pers_epochs&#125;</span>_<span class="subst">&#123;algo&#125;</span>.html&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h3 id="Per-FeAVG源码分析——data目录下：">Per_FeAVG源码分析——data目录下：</h3>
<h4 id="init-py"><a target="_blank" rel="noopener" href="http://init.py">init.py</a></h4>
<p>不做分析</p>
<h4 id="utils-py-2"><a target="_blank" rel="noopener" href="http://utils.py">utils.py</a></h4>
<h5 id="字典：DATASET-DICT">字典：DATASET_DICT</h5>
<p>作用：它将字符串键（如 <code>&quot;mnist&quot;</code> 和 <code>&quot;cifar&quot;</code>）映射到对应的类（<code>MNISTDataset</code> 和 <code>CIFARDataset</code>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATASET_DICT = &#123;</span><br><span class="line">    <span class="string">&quot;mnist&quot;</span>: MNISTDataset,</span><br><span class="line">    <span class="string">&quot;cifar&quot;</span>: CIFARDataset,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="函数：CURRENT-DIR">函数：CURRENT_DIR</h5>
<p>作用：<code>CURRENT_DIR</code> 被设置为当前 Python 脚本文件的父目录的绝对路径</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CURRENT_DIR = Path(__file__).parent.abspath()</span><br></pre></td></tr></table></figure>
<h5 id="函数：get-dataloader">函数：get_dataloader</h5>
<p>作用：从一个预处理好的 pickle 文件中加载数据集，并根据给定的 <code>client_id</code> 分割为训练集和验证集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader</span>(<span class="params">dataset: <span class="built_in">str</span>, client_id: <span class="built_in">int</span>, batch_size=<span class="number">20</span>, valset_ratio=<span class="number">0.1</span></span>):</span><br><span class="line">    pickles_dir = CURRENT_DIR / dataset / <span class="string">&quot;pickles&quot;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.isdir(pickles_dir) <span class="keyword">is</span> <span class="literal">False</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">&quot;Please preprocess and create pickles first.&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(pickles_dir / <span class="built_in">str</span>(client_id) + <span class="string">&quot;.pkl&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        client_dataset: DATASET_DICT[dataset] = pickle.load(f)</span><br><span class="line"></span><br><span class="line">    val_num_samples = <span class="built_in">int</span>(valset_ratio * <span class="built_in">len</span>(client_dataset))</span><br><span class="line">    train_num_samples = <span class="built_in">len</span>(client_dataset) - val_num_samples</span><br><span class="line"></span><br><span class="line">    trainset, valset = random_split(</span><br><span class="line">        client_dataset, [train_num_samples, val_num_samples]</span><br><span class="line">    )</span><br><span class="line">    trainloader = DataLoader(trainset, batch_size, drop_last=<span class="literal">True</span>)</span><br><span class="line">    valloader = DataLoader(valset, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trainloader, valloader</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h5 id="函数：get-client-id-indices-dataset">函数：get_client_id_indices(dataset)</h5>
<p>作用：从一个特定的 pickle 文件中加载并返回关于数据集分割的信息。从一个 <code>seperation.pkl</code> 文件中读取训练集、测试集以及总数目的索引或标识符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_client_id_indices</span>(<span class="params">dataset</span>):</span><br><span class="line">    dataset_pickles_path = CURRENT_DIR / dataset / <span class="string">&quot;pickles&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dataset_pickles_path / <span class="string">&quot;seperation.pkl&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        seperation = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> (seperation[<span class="string">&quot;train&quot;</span>], seperation[<span class="string">&quot;test&quot;</span>], seperation[<span class="string">&quot;total&quot;</span>])</span><br></pre></td></tr></table></figure>
<h4 id="preprocess-py"><a target="_blank" rel="noopener" href="http://preprocess.py">preprocess.py</a></h4>
<h5 id="函数：CURRENT-DIR-2">函数：CURRENT_DIR</h5>
<p>作用：<code>CURRENT_DIR</code> 被设置为当前 Python 脚本文件的父目录的绝对路径</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CURRENT_DIR = Path(__file__).parent.abspath()</span><br></pre></td></tr></table></figure>
<h5 id="字典：DATASET">字典：DATASET</h5>
<p>作用：数据集名称映射到了两个元组，可以基于数据集名称来动态地加载和实例化相应的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATASET = &#123;</span><br><span class="line">    <span class="string">&quot;mnist&quot;</span>: (MNIST, MNISTDataset),</span><br><span class="line">    <span class="string">&quot;cifar&quot;</span>: (CIFAR10, CIFARDataset),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="字典：MEAN">字典：MEAN</h5>
<p>作用：用来存储不同数据集的像素均值。这些均值通常用于数据归一化，只包含一个灰度通道，因此其均值是一个单元素元组 <code>(0.1307,)</code>。这意味着当你对 MNIST 数据集进行归一化时，你会从每个像素值中减去 0.1307。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MEAN = &#123;</span><br><span class="line">    <span class="string">&quot;mnist&quot;</span>: (<span class="number">0.1307</span>,),</span><br><span class="line">    <span class="string">&quot;cifar&quot;</span>: (<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="字典：STD">字典：STD</h5>
<p>作用：存储不同数据集的像素标准差。使用归一化时，标准差通常与均值一起使用，以确保数据的每个特征（在这个案例中是像素值）都有相似的尺度。它只包含一个灰度通道，因此其标准差是一个单元素元组 <code>(0.3015,)</code>。这意味着在归一化 MNIST 数据时，每个像素值都会根据其灰度通道的标准差进行缩放。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">STD = &#123;</span><br><span class="line">    <span class="string">&quot;mnist&quot;</span>: (<span class="number">0.3015</span>,),</span><br><span class="line">    <span class="string">&quot;cifar&quot;</span>: (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="函数：preprocess">函数：preprocess()</h5>
<p>作用：用于预处理数据集，在联邦学习或分布式学习的场景中，数据需要在多个客户端（或节点）之间分配。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">args: Namespace</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="comment">#参数和目录设置：</span></span><br><span class="line">    <span class="comment">#设置数据集目录（dataset_dir）和pickle文件目录（pickles_dir）。</span></span><br><span class="line">    dataset_dir = CURRENT_DIR / args.dataset</span><br><span class="line">    pickles_dir = CURRENT_DIR / args.dataset / <span class="string">&quot;pickles&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#设置随机数生成器的种子，以确保结果的可重复性。</span></span><br><span class="line">    np.random.seed(args.seed)</span><br><span class="line">    random.seed(args.seed)</span><br><span class="line">    torch.manual_seed(args.seed)</span><br><span class="line">    num_train_clients = <span class="built_in">int</span>(args.client_num_in_total * args.fraction)</span><br><span class="line">    num_test_clients = args.client_num_in_total - num_train_clients</span><br><span class="line"></span><br><span class="line">    <span class="comment">#数据转换</span></span><br><span class="line">    <span class="comment">#定义了一个转换transform，它只包含标准化（假设MEAN和STD是预定义的字典，包含了每个数据集的均值和标准差），初始化了训练集和测试集的统计信息字典</span></span><br><span class="line">    transform = transforms.Compose(</span><br><span class="line">        [transforms.Normalize(MEAN[args.dataset], STD[args.dataset]),]</span><br><span class="line">    )</span><br><span class="line">    target_transform = <span class="literal">None</span></span><br><span class="line">    trainset_stats = &#123;&#125;</span><br><span class="line">    testset_stats = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#目录和文件处理：</span></span><br><span class="line">    <span class="comment">#检查数据集目录是否存在，如果不存在则创建它。</span></span><br><span class="line">    <span class="comment">#如果pickle目录已经存在，则删除它（可能是为了确保没有旧的pickle文件干扰）。</span></span><br><span class="line">    <span class="comment">#创建新的pickle目录。</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(CURRENT_DIR / args.dataset):</span><br><span class="line">        os.mkdir(CURRENT_DIR / args.dataset)</span><br><span class="line">    <span class="keyword">if</span> os.path.isdir(pickles_dir):</span><br><span class="line">        os.system(<span class="string">f&quot;rm -rf <span class="subst">&#123;pickles_dir&#125;</span>&quot;</span>)</span><br><span class="line">    os.mkdir(<span class="string">f&quot;<span class="subst">&#123;pickles_dir&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#加载数据集</span></span><br><span class="line">    <span class="comment">#从预定义的DATASET字典中获取原始和目标数据集。</span></span><br><span class="line">    <span class="comment">#使用ori_dataset类创建训练集和测试集。注意，训练集在加载时还指定了download=True（用于自动下载数据集），而测试集没有。两者都使用了transforms.ToTensor()进行初步的数据转换。</span></span><br><span class="line">    ori_dataset, target_dataset = DATASET[args.dataset]</span><br><span class="line">    trainset = ori_dataset(</span><br><span class="line">        dataset_dir, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor()</span><br><span class="line">    )</span><br><span class="line">    testset = ori_dataset(dataset_dir, train=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">    <span class="comment">#分配类别到客户端</span></span><br><span class="line">    <span class="comment">#根据args.classes确定每个客户端应有的类别数量（默认为10类）</span></span><br><span class="line">    <span class="comment">#使用randomly_alloc_classes函数将类别随机分配给训练集和测试集的客户端</span></span><br><span class="line">    <span class="comment">#randomly_alloc_classes函数还返回每个子集的统计信息（</span></span><br><span class="line">    num_classes = <span class="number">10</span> <span class="keyword">if</span> args.classes &lt;= <span class="number">0</span> <span class="keyword">else</span> args.classes</span><br><span class="line">    all_trainsets, trainset_stats = randomly_alloc_classes(</span><br><span class="line">        ori_dataset=trainset,</span><br><span class="line">        target_dataset=target_dataset,</span><br><span class="line">        num_clients=num_train_clients,</span><br><span class="line">        num_classes=num_classes,</span><br><span class="line">        transform=transform,</span><br><span class="line">        target_transform=target_transform,</span><br><span class="line">    )</span><br><span class="line">    all_testsets, testset_stats = randomly_alloc_classes(</span><br><span class="line">        ori_dataset=testset,</span><br><span class="line">        target_dataset=target_dataset,</span><br><span class="line">        num_clients=num_test_clients,</span><br><span class="line">        num_classes=num_classes,</span><br><span class="line">        transform=transform,</span><br><span class="line">        target_transform=target_transform,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将所有训练集和测试集组合到一个列表all_datasets中</span></span><br><span class="line">    all_datasets = all_trainsets + all_testsets</span><br><span class="line"></span><br><span class="line">    <span class="comment">#保存客户端数据集为pickle文件</span></span><br><span class="line">    <span class="comment">#通过enumerate(all_datasets)遍历all_datasets列表中的每个数据集和对应的client_id（即客户端的ID）。</span></span><br><span class="line">    <span class="comment">#使用pathlib的/操作符（如果pickles_dir是pathlib.Path对象）来构建pickle文件的路径。</span></span><br><span class="line">    <span class="comment">#使用pickle.dump()函数将每个数据集保存到对应的pickle文件中。</span></span><br><span class="line">    <span class="keyword">for</span> client_id, dataset <span class="keyword">in</span> <span class="built_in">enumerate</span>(all_datasets):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(pickles_dir / <span class="built_in">str</span>(client_id) + <span class="string">&quot;.pkl&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(dataset, f)</span><br><span class="line">     </span><br><span class="line">    <span class="comment">#保存客户端索引</span></span><br><span class="line">    <span class="comment">#创建一个字典，其中包含三个键：“train”、“test”和“total”，“total”的值就是总的客户端数量。</span></span><br><span class="line">    <span class="comment">#“train”和“test”的值是客户端ID的列表，分别代表训练集和测试集的客户端。这里假设num_train_clients表示训练集客户端的数量，而args.client_num_in_total表示总的客户端数量。</span></span><br><span class="line">    <span class="comment">#使用pickle.dump()函数将这个字典保存到名为“seperation.pkl”的文件中。这个文件用于在后续的训练和测试过程中区分哪些客户端是训练集，哪些是测试集</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(pickles_dir / <span class="string">&quot;seperation.pkl&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">&quot;train&quot;</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train_clients)],</span><br><span class="line">                <span class="string">&quot;test&quot;</span>: [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train_clients, args.client_num_in_total)],</span><br><span class="line">                <span class="string">&quot;total&quot;</span>: args.client_num_in_total,</span><br><span class="line">            &#125;,</span><br><span class="line">            f,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#保存数据集统计信息</span></span><br><span class="line">    <span class="comment">#trainset_stats和testset_stats是在之前的预处理步骤中收集的训练集和测试集的统计信息。</span></span><br><span class="line">    <span class="comment">#使用json.dump()函数将这些统计信息保存为JSON格式的文件“all_stats.json”。这个文件用于在后续的模型训练和评估过程中提供数据集的相关信息</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(dataset_dir / <span class="string">&quot;all_stats.json&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(&#123;<span class="string">&quot;train&quot;</span>: trainset_stats, <span class="string">&quot;test&quot;</span>: testset_stats&#125;, f)</span><br></pre></td></tr></table></figure>
<h5 id="函数：randomly-alloc-classes">函数：randomly_alloc_classes</h5>
<p>作用：将原始数据集（<code>ori_dataset</code>）中的样本随机分配给多个客户端（或用户），同时确保每个客户端获得指定数量的不同类别的样本。函数还返回了分配给每个客户端的数据集列表和相应的统计信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">randomly_alloc_classes</span>(<span class="params"></span></span><br><span class="line"><span class="params">    ori_dataset: Dataset,</span></span><br><span class="line"><span class="params">    target_dataset: Dataset,</span></span><br><span class="line"><span class="params">    num_clients: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    num_classes: <span class="built_in">int</span>,</span></span><br><span class="line"><span class="params">    transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    target_transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[<span class="type">List</span>[Dataset], <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="type">Dict</span>[<span class="built_in">str</span>, <span class="built_in">int</span>]]]:</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#分配样本</span></span><br><span class="line">    <span class="comment">#使用noniid_slicing函数来将ori_dataset中的样本分配给num_clients个客户端。这个函数应该返回一个字典，其中键是客户端ID，值是分配给该客户端的样本索引列表。</span></span><br><span class="line">    dict_users = noniid_slicing(ori_dataset, num_clients, num_clients * num_classes)</span><br><span class="line">    stats = &#123;&#125;</span><br><span class="line">    <span class="comment">#收集统计信息</span></span><br><span class="line">    <span class="comment">#对于每个客户端，从ori_dataset中提取标签（ori_dataset.targets），然后根据分配给该客户端的样本索引列表计算标签的类别分布。这些统计信息被存储在stats字典中。</span></span><br><span class="line">    <span class="keyword">for</span> i, indices <span class="keyword">in</span> dict_users.items():</span><br><span class="line">        targets_numpy = np.array(ori_dataset.targets)</span><br><span class="line">        stats[<span class="string">f&quot;client <span class="subst">&#123;i&#125;</span>&quot;</span>] = &#123;<span class="string">&quot;x&quot;</span>: <span class="number">0</span>, <span class="string">&quot;y&quot;</span>: &#123;&#125;&#125;</span><br><span class="line">        stats[<span class="string">f&quot;client <span class="subst">&#123;i&#125;</span>&quot;</span>][<span class="string">&quot;x&quot;</span>] = <span class="built_in">len</span>(indices)</span><br><span class="line">        stats[<span class="string">f&quot;client <span class="subst">&#123;i&#125;</span>&quot;</span>][<span class="string">&quot;y&quot;</span>] = Counter(targets_numpy[indices].tolist())</span><br><span class="line">    datasets = []</span><br><span class="line">    <span class="comment">#创建数据集</span></span><br><span class="line">    <span class="comment">#使用target_dataset类从ori_dataset中创建子集，每个子集对应于一个客户端。这是通过从ori_dataset中提取分配给该客户端的样本，并传递给target_dataset的构造函数来实现的。</span></span><br><span class="line">    <span class="keyword">for</span> indices <span class="keyword">in</span> dict_users.values():</span><br><span class="line">        datasets.append(</span><br><span class="line">            target_dataset(</span><br><span class="line">                [ori_dataset[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices],</span><br><span class="line">                transform=transform,</span><br><span class="line">                target_transform=target_transform,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> datasets, stats</span><br></pre></td></tr></table></figure>
<h5 id="函数：-name-“main”">函数：__name__==“main”</h5>
<p>作用：基本的命令行参数解析设置，它使用<code>argparse</code>库来从命令行获取参数。这些参数包括数据集类型、客户端总数、训练客户端的比例、每个客户端数据所属的类别数量以及随机种子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = ArgumentParser()</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--dataset&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=[<span class="string">&quot;mnist&quot;</span>, <span class="string">&quot;cifar&quot;</span>], default=<span class="string">&quot;mnist&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--client_num_in_total&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">200</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--fraction&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.9</span>, <span class="built_in">help</span>=<span class="string">&quot;Propotion of train clients&quot;</span></span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">&quot;--classes&quot;</span>,</span><br><span class="line">        <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">        default=<span class="number">2</span>,</span><br><span class="line">        <span class="built_in">help</span>=<span class="string">&quot;Num of classes that one client&#x27;s data belong to.&quot;</span>,</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--seed&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    preprocess(args)</span><br></pre></td></tr></table></figure>
<h4 id="dataset-py"><a target="_blank" rel="noopener" href="http://dataset.py">dataset.py</a></h4>
<h5 id="类：MNISTDataset-Dataset">类：MNISTDataset(Dataset)</h5>
<h6 id="函数：init-2">函数：init</h6>
<p>作用：用于初始化一个对象的状态</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">    <span class="comment">#参数</span></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    subset=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    data=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    targets=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    target_transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="comment">#处理</span></span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br><span class="line">    <span class="comment">#如果data和targets都非空，它将data增加一个新的维度（使用unsqueeze(1)），并将data和targets设置为对象的属性。</span></span><br><span class="line">    <span class="keyword">if</span> (data <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        self.data = data.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        self.targets = targets</span><br><span class="line">    <span class="comment"># 如果subset非空，它将遍历subset，检查元组中的每个元素是否为张量。如果不是，它将使用torch.tensor将其转换为张量。然后，它使用torch.stack将数据和标签分别堆叠成张量，并设置为对象的属性。   </span></span><br><span class="line">    <span class="keyword">elif</span> subset <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.data = torch.stack(</span><br><span class="line">            <span class="built_in">list</span>(</span><br><span class="line">                <span class="built_in">map</span>(</span><br><span class="line">                    <span class="keyword">lambda</span> tup: tup[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">isinstance</span>(tup[<span class="number">0</span>], torch.Tensor)</span><br><span class="line">                    <span class="keyword">else</span> torch.tensor(tup[<span class="number">0</span>]),</span><br><span class="line">                    subset,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        self.targets = torch.stack(</span><br><span class="line">            <span class="built_in">list</span>(</span><br><span class="line">                <span class="built_in">map</span>(</span><br><span class="line">                    <span class="keyword">lambda</span> tup: tup[<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">isinstance</span>(tup[<span class="number">1</span>], torch.Tensor)</span><br><span class="line">                    <span class="keyword">else</span> torch.tensor(tup[<span class="number">1</span>]),</span><br><span class="line">                    subset,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    <span class="comment">#如果data和targets以及subset都为空，则抛出一个ValueError，说明需要提供数据格式。</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;Data Format: subset: Tuple(data: Tensor / Image / np.ndarray, targets: Tensor) OR data: List[Tensor]  targets: List[Tensor]&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h6 id="函数：getitem">函数：getitem</h6>
<p>作用：允许类的实例像列表、元组或其他可迭代对象那样进行索引访问。在你提供的上下文中，这个方法通常用于数据加载器（如PyTorch的<code>DataLoader</code>），以便在训练或评估模型时能够按索引访问数据集中的单个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">    data, targets = self.data[index], self.targets[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        data = self.transform(self.data[index])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        targets = self.target_transform(self.targets[index])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data, targets</span><br></pre></td></tr></table></figure>
<h6 id="函数：len">函数：len</h6>
<p>作用：确定self.data的长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.targets)</span><br></pre></td></tr></table></figure>
<h5 id="类：CIFARDataset-Dataset">类：CIFARDataset(Dataset)</h5>
<h6 id="函数：init-3">函数：init</h6>
<p>作用：用于初始化一个对象的状态</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">    self,</span></span><br><span class="line"><span class="params">    subset=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    data=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    targets=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    target_transform=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    self.transform = transform</span><br><span class="line">    self.target_transform = target_transform</span><br><span class="line">    <span class="keyword">if</span> (data <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>) <span class="keyword">and</span> (targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>):</span><br><span class="line">        self.data = data.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        self.targets = targets</span><br><span class="line">    <span class="keyword">elif</span> subset <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.data = torch.stack(</span><br><span class="line">            <span class="built_in">list</span>(</span><br><span class="line">                <span class="built_in">map</span>(</span><br><span class="line">                    <span class="keyword">lambda</span> tup: tup[<span class="number">0</span>]</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">isinstance</span>(tup[<span class="number">0</span>], torch.Tensor)</span><br><span class="line">                    <span class="keyword">else</span> torch.tensor(tup[<span class="number">0</span>]),</span><br><span class="line">                    subset,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        self.targets = torch.stack(</span><br><span class="line">            <span class="built_in">list</span>(</span><br><span class="line">                <span class="built_in">map</span>(</span><br><span class="line">                    <span class="keyword">lambda</span> tup: tup[<span class="number">1</span>]</span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">isinstance</span>(tup[<span class="number">1</span>], torch.Tensor)</span><br><span class="line">                    <span class="keyword">else</span> torch.tensor(tup[<span class="number">1</span>]),</span><br><span class="line">                    subset,</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(</span><br><span class="line">            <span class="string">&quot;Data Format: subset: Tuple(data: Tensor / Image / np.ndarray, targets: Tensor) OR data: List[Tensor]  targets: List[Tensor]&quot;</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
<h6 id="函数：getitem-2">函数：getitem</h6>
<p>作用：允许类的实例像列表、元组或其他可迭代对象那样进行索引访问。在你提供的上下文中，这个方法通常用于数据加载器（如PyTorch的<code>DataLoader</code>），以便在训练或评估模型时能够按索引访问数据集中的单个样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">    img, targets = self.data[index], self.targets[index]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        img = self.transform(self.data[index])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        targets = self.target_transform(self.targets[index])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img, targets</span><br></pre></td></tr></table></figure>
<h6 id="函数：len-2">函数：len</h6>
<p>作用：返回长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(self.targets)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://longjinw.github.io">龙金伟</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://longjinw.github.io/2024/06/27/Per-FedAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">https://longjinw.github.io/2024/06/27/Per-FedAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://longjinw.github.io" target="_blank">龙锦</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Per-FedAVG/">Per_FedAVG</a></div><div class="post_share"><div class="social-share" data-image="/image/T1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/27/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E9%9B%86%E5%90%88/" title="Linux常用命令集合"><img class="cover" src="/image/T1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Linux常用命令集合</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/27/shell%E8%84%9A%E6%9C%AC%E7%BC%96%E5%86%99%E5%9F%BA%E7%A1%80/" title="shell脚本编写基础"><img class="cover" src="/image/T2.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">shell脚本编写基础</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/06/28/Per-FedAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E6%80%BB%E9%A2%86/" title="Per_FedAVG源码分析总领"><img class="cover" src="/image/T1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-28</div><div class="title">Per_FedAVG源码分析总领</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/luogic.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">龙金伟</div><div class="author-info__description">买不起茅台 </div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/longjinw"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/longjinw" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="https://blog.csdn.net/longer_net" target="_blank" title="csdn"><i class="fa-regular fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Per-FeAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E6%A0%B9%E7%9B%AE%E5%BD%95%E4%B8%8B%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">Per_FeAVG源码分析——根目录下：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#utils-py"><span class="toc-number">1.1.</span> <span class="toc-text">utils.py</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aget-args%EF%BC%88%EF%BC%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">函数：get_args（）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aeval%EF%BC%88%EF%BC%89"><span class="toc-number">1.1.2.</span> <span class="toc-text">函数：eval（）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Afix-random-seed-seed-int"><span class="toc-number">1.1.3.</span> <span class="toc-text">函数：fix_random_seed(seed: int)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#model-py"><span class="toc-number">1.2.</span> <span class="toc-text">model.py</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aelu-nn-Module"><span class="toc-number">1.2.1.</span> <span class="toc-text">函数：elu(nn.Module)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%EF%BC%9Alinear-nn-Module"><span class="toc-number">1.2.2.</span> <span class="toc-text">类：linear(nn.Module)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%EF%BC%9AMLP-MNIST"><span class="toc-number">1.2.3.</span> <span class="toc-text">类：MLP_MNIST</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%EF%BC%9AMLP-CIFAR10"><span class="toc-number">1.2.4.</span> <span class="toc-text">类：MLP_CIFAR10</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%97%E5%85%B8%EF%BC%9AMODEL-DICT"><span class="toc-number">1.2.5.</span> <span class="toc-text">字典：MODEL_DICT</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aget-model-dataset-device"><span class="toc-number">1.2.6.</span> <span class="toc-text">函数：get_model(dataset, device)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#perfedavg-py"><span class="toc-number">1.3.</span> <span class="toc-text">perfedavg.py</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Ainit"><span class="toc-number">1.3.1.</span> <span class="toc-text">函数：init()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aget-data-batch-self"><span class="toc-number">1.3.2.</span> <span class="toc-text">函数：get_data_batch(self)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Atrain%EF%BC%88%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">函数：train（）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9A-train"><span class="toc-number">1.3.4.</span> <span class="toc-text">函数：_train()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Acompute-grad"><span class="toc-number">1.3.5.</span> <span class="toc-text">函数：compute_grad()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Apers-N-eval"><span class="toc-number">1.3.6.</span> <span class="toc-text">函数：pers_N_eval()</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#main-py"><span class="toc-number">1.4.</span> <span class="toc-text">main.py</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Per-FeAVG%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94%E2%80%94data%E7%9B%AE%E5%BD%95%E4%B8%8B%EF%BC%9A"><span class="toc-number">2.</span> <span class="toc-text">Per_FeAVG源码分析——data目录下：</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#init-py"><span class="toc-number">2.1.</span> <span class="toc-text">init.py</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#utils-py-2"><span class="toc-number">2.2.</span> <span class="toc-text">utils.py</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%97%E5%85%B8%EF%BC%9ADATASET-DICT"><span class="toc-number">2.2.1.</span> <span class="toc-text">字典：DATASET_DICT</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9ACURRENT-DIR"><span class="toc-number">2.2.2.</span> <span class="toc-text">函数：CURRENT_DIR</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aget-dataloader"><span class="toc-number">2.2.3.</span> <span class="toc-text">函数：get_dataloader</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Aget-client-id-indices-dataset"><span class="toc-number">2.2.4.</span> <span class="toc-text">函数：get_client_id_indices(dataset)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#preprocess-py"><span class="toc-number">2.3.</span> <span class="toc-text">preprocess.py</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9ACURRENT-DIR-2"><span class="toc-number">2.3.1.</span> <span class="toc-text">函数：CURRENT_DIR</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%97%E5%85%B8%EF%BC%9ADATASET"><span class="toc-number">2.3.2.</span> <span class="toc-text">字典：DATASET</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%97%E5%85%B8%EF%BC%9AMEAN"><span class="toc-number">2.3.3.</span> <span class="toc-text">字典：MEAN</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AD%97%E5%85%B8%EF%BC%9ASTD"><span class="toc-number">2.3.4.</span> <span class="toc-text">字典：STD</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Apreprocess"><span class="toc-number">2.3.5.</span> <span class="toc-text">函数：preprocess()</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Arandomly-alloc-classes"><span class="toc-number">2.3.6.</span> <span class="toc-text">函数：randomly_alloc_classes</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9A-name-%E2%80%9Cmain%E2%80%9D"><span class="toc-number">2.3.7.</span> <span class="toc-text">函数：__name__&#x3D;&#x3D;“main”</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dataset-py"><span class="toc-number">2.4.</span> <span class="toc-text">dataset.py</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%EF%BC%9AMNISTDataset-Dataset"><span class="toc-number">2.4.1.</span> <span class="toc-text">类：MNISTDataset(Dataset)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Ainit-2"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">函数：init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Agetitem"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">函数：getitem</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Alen"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">函数：len</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%B1%BB%EF%BC%9ACIFARDataset-Dataset"><span class="toc-number">2.4.2.</span> <span class="toc-text">类：CIFARDataset(Dataset)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Ainit-3"><span class="toc-number">2.4.2.1.</span> <span class="toc-text">函数：init</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Agetitem-2"><span class="toc-number">2.4.2.2.</span> <span class="toc-text">函数：getitem</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%EF%BC%9Alen-2"><span class="toc-number">2.4.2.3.</span> <span class="toc-text">函数：len</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/04/C-%E5%B8%B8%E8%A7%81%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="C++常见设计模式">C++常见设计模式</a><time datetime="2024-07-04T03:44:20.000Z" title="发表于 2024-07-04 11:44:20">2024-07-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/04/C-I-O%E8%BF%9B%E7%A8%8B%E6%8A%80%E6%9C%AF/" title="C++I/O进程技术">C++I/O进程技术</a><time datetime="2024-07-04T03:44:04.000Z" title="发表于 2024-07-04 11:44:04">2024-07-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/03/C-%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7%E7%9B%B8%E5%85%B3/" title="C++语言特性相关"><img src="/iamge/T2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++语言特性相关"/></a><div class="content"><a class="title" href="/2024/07/03/C-%E8%AF%AD%E8%A8%80%E7%89%B9%E6%80%A7%E7%9B%B8%E5%85%B3/" title="C++语言特性相关">C++语言特性相关</a><time datetime="2024-07-03T10:41:30.000Z" title="发表于 2024-07-03 18:41:30">2024-07-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/03/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E7%89%B9%E6%80%A7/" title="C++面向对象的特性"><img src="/image/T1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++面向对象的特性"/></a><div class="content"><a class="title" href="/2024/07/03/C-%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%9A%84%E7%89%B9%E6%80%A7/" title="C++面向对象的特性">C++面向对象的特性</a><time datetime="2024-07-03T07:13:01.000Z" title="发表于 2024-07-03 15:13:01">2024-07-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/02/C-%E5%85%B3%E9%94%AE%E5%AD%97%E5%92%8C%E5%85%B3%E9%94%AE%E5%BA%93%E5%87%BD%E6%95%B0%E7%BB%86%E8%AE%BA/" title="C++关键字和关键库函数细论"><img src="/image/T2.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++关键字和关键库函数细论"/></a><div class="content"><a class="title" href="/2024/07/02/C-%E5%85%B3%E9%94%AE%E5%AD%97%E5%92%8C%E5%85%B3%E9%94%AE%E5%BA%93%E5%87%BD%E6%95%B0%E7%BB%86%E8%AE%BA/" title="C++关键字和关键库函数细论">C++关键字和关键库函数细论</a><time datetime="2024-07-02T11:43:08.000Z" title="发表于 2024-07-02 19:43:08">2024-07-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By 龙金伟</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/xiabo2/CDN@latest/fishes.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>